<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>NOAH</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/teaser.png"/>
  	<meta property="og:title" content="Neural Prompt Search." />
  	<meta property="og:description" content="Searching prompt modules for parameter-efficient transfer learning.." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Neural Prompt Search." />
    <meta property="twitter:description"   content="Searching prompt modules for parameter-efficient transfer learning.." />
    <meta property="twitter:image"         content="./resources/teaser.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        Neural Prompt Search
    </div>

    <!-- <div class="venue">
        arXiv
    </div> -->

    <br><br>

    <div class="author">
        <a href="https://davidzhangyuanhan.github.io/">Yuanhan Zhang</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://kaiyangzhou.github.io/">Kaiyang Zhou</a><sup>1</sup>
    </div>


    <div class="author">
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>1</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>S-Lab, Nanyang Technological University</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/abs/2206.04673">[Paper]</a></div>
    <!-- <div class="links"><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">[Video]</a></div> -->
    <div class="links"><a href="https://github.com/Davidzhangyuanhan/NOAH">[Code]</a></div>

    <br><br>

    <!-- <table class="center" style="width: 100%;margin-right:auto;margin-left:auto;" > -->

    <!-- </table> -->

    <!-- <img style="width: 80%;" src="./resources/teaser.png" alt="Teaser figure."/> -->
    <!-- <br>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p> -->

    <br><br>
    <hr>

    <h1>TL;DR</h1>
    <p style="width: 80%;">
        The idea is simple: we view existing parameter-efficient tuning modules, including <a href="https://arxiv.org/abs/1902.00751">Adapter</a>, <a href="https://arxiv.org/abs/2106.09685">LoRA</a> and <a href="https://arxiv.org/abs/2203.12119">VPT</a>, as prompt modules and propose to search the optimal configuration via neural architecture search. Our approach is named NOAH (Neural prOmpt seArcH).
    </p>
    
    <!-- <div onmouseout="noah_stop()" onmouseover="noah_start()">
        <!-- <td style="padding:20px;width:45%;vertical-align:middle"> -->
        <!-- <div id="noah_anim" class="hidden" style="display: none;"><img src="./resources/NOAH_annimation.gif" style="width: 80%;"></div> -->
        <!-- <div id="noah_still" style="display: none;"><img src="./resources/teaser.png" style="width: 80%;"></div> -->
        <!-- <script type="text/javascript">
            function noah_start() {
            document.getElementById('noah_anim').style.display = 'inline';
            document.getElementById('noah_still').style.display = 'none';
            }

            function noah_stop() {
            document.getElementById('noah_anim').style.display = 'none';
            document.getElementById('noah_still').style.display = 'inline';
            }
            noah_stop()
        </script> -->
    <!-- </div> --> 


    <img id="gif-2" src="./resources/teaser.png" 
        onmouseover="document.getElementById('gif-2').src='./resources/NOAH_annimation.gif' " 
        onmouseout="document.getElementById('gif-2').src='./resources/teaser.png'" 
        style="width: 80%;"

    />

    <br><br>
    <hr>


    <h1>Abstract</h1>
    <p style="width: 80%;">
        The size of vision models has grown exponentially over the last few years, especially after the emergence of Vision Transformer. 
        This has motivated the development of parameter-efficient tuning methods, such as learning adapter layers or visual prompt tokens, 
        which allow a tiny portion of model parameters to be trained whereas the vast majority obtained from pre-training are frozen. 
        However, designing a proper tuning method is non-trivial: one might need to try out a lengthy list of design choices, 
        not to mention that each downstream dataset often requires custom designs. In this paper, we view the existing parameter-efficient tuning methods 
        as ``prompt modules'' and propose <strong>Neural prOmpt seArcH (NOAH)</strong>, a novel approach that learns, 
        for large vision models, the optimal design of prompt modules through a neural architecture search algorithm, 
        specifically for each downstream dataset. By conducting extensive experiments on over 20 vision datasets, 
        we demonstrate that NOAH (i) is superior to individual prompt modules, (ii) has good few-shot learning ability, and (iii) is domain-generalizable. 
    </p>

    <br><br>
    <hr>

    <!-- <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>
    <hr> -->

    <!-- <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method.jpg"
         alt="Method overview figure"/>
    <br>
    <a class="links" href="https://github.com/elliottwu/webpage-template">[Code]</a>

    <br><br>
    <hr> -->

    <!-- <h1>Results</h1>
    <img style="width: 80%;" src="./resources/results.jpg"
         alt="Results figure"/>

    <br><br>
    <hr> -->

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org">
            <img class="layered-paper-big" width="100%" src="./resources/paper.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>Neural Prompt Search</h3>
        <p>Yuanhan Zhang, Kaiyang Zhou and Ziwei Liu</p>
        <p>arXiv, 2022.</p>
        <pre><code>@InProceedings{zhang2022NOAH,
    title = {Neural Prompt Search},
    author = {Yuanhan Zhang and Kaiyang Zhou and Ziwei Liu},
    archivePrefix={arXiv},
    year = {2022},
}</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        TBA.
    </p>

    <br><br>
</div>

</body>

</html>
